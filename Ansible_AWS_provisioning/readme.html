<h1 id="ansible-aws-provisioning">Ansible AWS provisioning</h1>
<p>This is a suite of Ansible playbooks to provision an entire AWS infrastructure with a Staging instance and an auto-scaled load-balanced Production environment, and to deploy a webapp thereon. First the EC2 SSH key and Security Groups are created, then a Staging instance is provisioned, then the webapp is deployed on Staging from GitHub, then an image is taken from which to provision the Production environment. The Production environment is set up with auto-scaled EC2 instances running behind a load balancer. Finally, DNS entries are added for the Production and Staging environments.</p>
<p>This is currently configured for very modest requirements, with a maximum of three t2.micro instances for auto-scaling, but it’s trivial to change these settings. There’s no reason why this set of playbooks shouldn’t handle scaling out to a much larger infrastructure, with more powerful and specialised instance types as needed.</p>
<p>I created a very basic <a href="https://github.com/mattbrock/simple_webapp">Python webapp</a> to use as an example for the deployment here, but you can replace that with your own webapp should you so wish.</p>
<h2 id="installationsetup">Installation/setup</h2>
<ol type="1">
<li>You’ll need an AWS account with a VPC set up, and with a DNS domain set up in Route 53.</li>
<li>Install and configure the latest version of the <a href="https://aws.amazon.com/cli/">AWS CLI</a>. The settings in the AWS CLI configuration files are needed by the Ansible modules in these playbooks. Also, the Ansible modules don’t yet support target tracking auto-scaling policies, so there is one task which needs to run the AWS CLI as a local external command for that purpose. If you’re using a Mac, I’d recommend using <a href="https://brew.sh/">Homebrew</a> as the simplest way of installing and managing the AWS CLI.</li>
<li>If you don’t already have it, you’ll need Python3. You’ll also need the boto and boto3 Python modules (for Ansible modules and dynamic inventory) which can be installed via pip.</li>
<li><a href="https://www.ansible.com/">Ansible</a> needs to be installed and configured. Again, if you’re on a Mac, using Homebrew for this is probably best.</li>
<li>Copy <em><a href="etc/variables_template.yml">etc/variables_template.yml</a></em> to <em>etc/variables.yml</em> and update the static variables at the top for your own environment setup.</li>
</ol>
<h2 id="usage">Usage</h2>
<p>These playbooks are run in the standard way, i.e. <code>ansible-playbook PLAYBOOK_NAME.yml</code>. Note that Step 3 also requires the addition of <code>-i etc/inventory.aws_ec2.yml</code> to use the dynamic inventory.</p>
<p>To deploy your own webapp instead of my <a href="https://github.com/mattbrock/simple_webapp">basic Python app</a>, you’ll need to edit <a href="deploy_staging.yml">deploy_staging.yml</a> so that Step 3 deploys your app with your own specific requirements, files, configuration, etc.</p>
<h2 id="playbooks-for-provisioningdeployment">Playbooks for provisioning/deployment</h2>
<ol type="1">
<li><a href="provision_key_sg.yml">provision_key_sg.yml</a> Provisions EC2 SSH key and Security Groups.</li>
<li><a href="provision_staging.yml">provision_staging.yml</a> Provisions a Staging instance based on the official Amazon Linux 2 AMI.</li>
<li><a href="deploy_staging.yml">deploy_staging.yml</a> Sets up a Staging instance and deploys the app on it.
<ol type="1">
<li>Requires dynamic inventory specification, so run with: <code>ansible-playbook -i etc/inventory.aws_ec2.yml deploy_staging.yml</code>.</li>
</ol></li>
<li><a href="image_staging.yml">image_staging.yml</a> Builds an AMI image from the Staging instance.</li>
<li><a href="provision_tg_elb.yml">provision_tg_elb.yml</a> Provisions a Target Group and Elastic Load Balancer ready for the Production environment.</li>
<li><a href="provision_production.yml">provision_production.yml</a> Provisions the auto-scaled Production environment from the Staging AMI, and attaches the Auto Scaling group to ELB Target Group.
<ol type="1">
<li>This playbook does not wait for instances to be deployed as specified, so it will take some time after the playbook runs before the additions/changes become apparent.</li>
</ol></li>
<li><a href="provision_dns.yml">provision_dns.yml</a> Provisions the DNS in Route 53 for the Production environment and the Staging instance.
<ol type="1">
<li>Note that it may take a few minutes for the DNS to propagate before it becomes usable.</li>
</ol></li>
</ol>
<p>Running later playbooks without having run the earlier ones will fail due to missing components and variables etc.</p>
<p>Running all seven playbooks in succession will set up the entire infrastructure from start to finish.</p>
<p>Once the infrastructure is up and running, any changes to the app can be redeployed to Staging by running Step 3 again. You would then run Step 4 and Step 6 to rebuild the Production environment from the updated Staging environment. Note that in this situation, the old instances in Production are replaced with new ones in a rolling fashion, so it will take a while before the old instances are terminated and the new ones are in place.</p>
<h2 id="playbooks-for-deprovisioning">Playbooks for deprovisioning</h2>
<ol type="1">
<li><a href="destroy_all.yml">destroy_all.yml</a> Destroys the entire infrastructure.</li>
<li><a href="delete_all.yml">delete_all.yml</a> Clears all dynamic variables in the <em>etc/variables.yml</em> file.</li>
</ol>
<p><strong>USE <em>destroy_all.yml</em> WITH EXTREME CAUTION!</strong> If your shell is configured for the wrong AWS account, you could potentially cause serious damage with this. Always check before running that your shell is configured for the correct environment and that you are absolutely 100 percent sure you want to do this. Don’t say I didn’t warn you!</p>
<p>Due to the fact that it might take some time to deprovision certain elements, some tasks in <em>destroy_all.yml</em> may initially fail. This should be nothing to worry about. If it happens, wait for a little while then run the playbook again until all tasks have succeeded.</p>
<p>Once everything has been fully destroyed, it’s safe to run the <em>delete_all.yml</em> playbook to clear out the variables file. Do not run this until you are sure everything has been fully destroyed, because the SSH key file can never be recovered again after it has been deleted.</p>
<h2 id="checking-the-staging-and-production-sites">Checking the Staging and Production sites</h2>
<p>To check the app on Staging once deployed in Step 3, you can get the Staging instance’s public DNS via the AWS CLI with this command:</p>
<p><code>aws ec2 describe-instances --filters "Name=tag:Environment,Values=Staging" --query "Reservations[*].Instances[*].PublicDnsName"</code></p>
<p>Then check it in your browser on port 8080 at http://ec2-xxx-xxx-xxx-xxx.xx-xxxx-x.compute.amazonaws.com:8080/ (replacing “ec2-xxx-xxx-xxx-xxx.xx-xxxx-x.compute.amazonaws.com” with the actual public address of the instance).</p>
<p>To check the app on Production once deployed in Step 6, you can get the ELB’s DNS name by grepping the <em>variables.yml</em> file:</p>
<p><code>grep elb_dns etc/variables.yml | cut -d " " -f 2</code></p>
<p>Then just check that in your web browser.</p>
<p>Once Step 7 has been run to create the DNS entries (and you’ve waited a little while for the DNS to propagate) you can visit your Production site at http://www.yourdomain.com/ and your Staging site at http://staging.yourdomain.com:8080/ (noting the use of port 80 for Staging, and obviously replacing “yourdomain.com” with your actual domain as specified in the <em>/etc/variables.yml</em> file).</p>
<h2 id="load-testing-to-check-auto-scaling-response">Load testing to check auto-scaling response</h2>
<p>If you don’t have enough traffic coming in to trigger an Auto Scaling event and you’re wondering if the scaling is working as intended, you can use a benchmarking tool such as Apache’s <a href="https://httpd.apache.org/docs/current/programs/ab.html">ab</a> to artifically create large amounts of incoming traffic. This should raise the load on the Production instance enough to trigger the automatic launch of an additional Production instance. I’ve found running this command from two separate servers is usually sufficient (if you don’t have any suitable servers, you can temporarily fire up a couple of EC2 instances for the purpose):</p>
<p><code>ab -c 250 -n 1000000 http://www.yourdomain.com/</code></p>
<p>This will simulate 250 simultaneous requests from each server, and will keep going until you cancel it (or until it hits a million requests, but an auto-scaling event should occur well before that number gets reached).</p>
<h2 id="connecting-to-instances-via-ssh">Connecting to instances via SSH</h2>
<p>If you need to SSH into the Staging instance once it’s running after Step 2, get the public DNS name using the command above, then SSH in with:</p>
<p><code>ssh -i etc/ec2_key.pem ec2-user@ec2-xxx-xxx-xxx-xxx.xx-xxxx-x.compute.amazonaws.com</code></p>
<p>If you need to SSH into the Production instances once they’re running after Step 6, get the list of public DNS names for the Production instances with this command (there may only be one instance):</p>
<p><code>aws ec2 describe-instances --filters "Name=tag:Environment,Values=Production" --query "Reservations[*].Instances[*].PublicDnsName"</code></p>
<p>Then connect via SSH in the same way as with the Staging instance.</p>
<h2 id="running-ad-hoc-ansible-commands">Running ad hoc Ansible commands</h2>
<p>To run ad hoc commands (e.g. <code>uptime</code> in this example) remotely with Ansible (without playbooks) you can use the <code>ansible</code> command as follows:</p>
<p><code>ansible -i etc/inventory.aws_ec2.yml -u ec2-user --private-key etc/ec2_key.pem tag_Environment_Staging -m shell -a uptime</code></p>
<p>That can be used for the Staging instance. To run the command on all the Production instances at once, replace “tag_Environment_Staging” with “tag_Environment_Production”.</p>
